---
title: MCP Server Observability & Monitoring - Phase 3 Completion
status: active
priority: high
date_created: 2026-01-24
date_updated: 2026-01-24
completion:
  - [x] Phase 3.1 - Datadog SDK Integration ‚úÖ
  - [x] Phase 3.2 - APM Tracing Implementation ‚úÖ
  - [ ] Phase 3.3 - Metrics Collection
  - [ ] Phase 3.4 - Infrastructure Monitoring
  - [ ] Phase 3.5 - Log Aggregation
  - [ ] Phase 3.6 - Dashboard Setup
  - [ ] Phase 3.7 - Alert Configuration
  - [ ] Phase 3.8 - Integration Testing
  - [ ] Phase 3.9 - Documentation Update
related_files:
  - src/server.py
  - src/config_loader.py
  - src/datadog_integration.py
  - src/redis_coordinator.py
  - src/sentry_integration.py
  - config/.env
  - tests/test_datadog_integration.py
  - tests/test_mcp_server.py
  - docs/IMPLEMENTATION.md
---

# MCP Server Observability & Monitoring - Phase 3 Completion

**Current Date**: January 24, 2026  
**Status**: Phase 1 ‚úÖ Complete | Phase 2 ‚úÖ Complete | Phase 3 üîÑ **IN PROGRESS** (Task 3.1 Complete)

## Executive Summary

This plan completes the MCP Server Enhancement roadmap by implementing **Datadog APM and Infrastructure Monitoring** (Phase 3). This gives AI agents comprehensive observability across applications, services, and infrastructure to analyze failures, performance bottlenecks, and system-wide patterns.

### Implementation Status (Ground Truth from Codebase)

| Phase | Feature | Implementation | Tests | Production |
|-------|---------|----------------|-------|------------|
| **Phase 1** | Redis Coordination | ‚úÖ 100% | ‚úÖ 4/4 pass | ‚úÖ Ready |
| **Phase 1** | Distributed Semaphore | ‚úÖ 100% | ‚úÖ Tested | ‚úÖ Ready |
| **Phase 1** | Shared Cache (500MB) | ‚úÖ 100% | ‚úÖ Tested | ‚úÖ Ready |
| **Phase 1** | Graceful Fallback | ‚úÖ 100% | ‚úÖ Tested | ‚úÖ Ready |
| **Phase 2** | Sentry SDK | ‚úÖ 100% | ‚úÖ Tested | ‚úÖ Ready |
| **Phase 2** | Per-Service DSNs | ‚úÖ 100% | ‚ö†Ô∏è Manual | ‚úÖ Ready |
| **Phase 2** | Sentry API Client | ‚úÖ 100% | ‚úÖ 1/1 pass | ‚úÖ Ready |
| **Phase 2** | Error Tracking | ‚úÖ 100% | ‚úÖ Tested | ‚úÖ Ready |
| **Phase 2** | Performance Tracking | ‚úÖ 100% | ‚úÖ Tested | ‚úÖ Ready |
| **Phase 2** | MCP Sentry Tools (3) | ‚úÖ 100% | ‚úÖ Tested | ‚úÖ Ready |
| **Phase 3** | Datadog SDK | ‚úÖ 100% | ‚úÖ 10/10 pass | ‚úÖ Ready |
| **Phase 3** | APM Tracing | ‚úÖ 100% | ‚úÖ 15/15 pass | ‚úÖ Ready |
| **Phase 3** | Metrics Collection | ‚ùå 0% | ‚ùå None | ‚ùå Not started |
| **Phase 3** | Infrastructure Monitoring | ‚ùå 0% | ‚ùå None | ‚ùå Not started |

**Test Coverage**: 46/46 tests passing ‚úÖ (31 existing + 10 Datadog SDK + 5 APM tracing)  
**Code Quality**: No TODOs/FIXMEs, proper async patterns, type hints ‚úÖ

---

## Architecture Overview

### Current Implementation (Phases 1-2 Complete)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AI Agent (GitHub Copilot / Claude / IntelliJ Junie)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ SSH + MCP (JSON-RPC over stdio)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LogAI MCP Server (Per-Session Process)                 ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ 5 MCP Tools:                                        ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ search_logs (multi-service, UTC timestamps)    ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ read_search_file (overflow results)            ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ query_sentry_issues ‚úÖ PHASE 2                 ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ get_sentry_issue_details ‚úÖ PHASE 2            ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ search_sentry_traces ‚úÖ PHASE 2                ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Redis Coordinator: ‚úÖ PHASE 1                       ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ Global semaphore (20 concurrent searches)      ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ Shared cache (500MB, 10min TTL)                ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ Graceful local fallback                        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Sentry Integration: ‚úÖ PHASE 2                      ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ Per-service DSN routing                        ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ Error capture with context                     ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ Performance tracking                           ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ API client (issues, traces, events)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Infrastructure Layer                                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Redis (localhost:6379) - Global coordination       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Sentry (sentry.io) - Error/performance tracking    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Log Files (300GB+/day) - Ripgrep/grep search       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ services.yaml (90+ services) - Service definitions ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Target State After Phase 3

```diff
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AI Agent (GitHub Copilot / Claude / IntelliJ Junie)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ SSH + MCP (JSON-RPC over stdio)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LogAI MCP Server (Per-Session Process)                 ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ 5 MCP Tools (existing) +                           ‚îÇ
+  ‚îú‚îÄ‚îÄ 4 NEW MCP Tools: üÜï PHASE 3                         ‚îÇ
+  ‚îÇ   ‚îú‚îÄ‚îÄ query_datadog_apm (service performance)        ‚îÇ
+  ‚îÇ   ‚îú‚îÄ‚îÄ get_datadog_traces (distributed tracing)       ‚îÇ
+  ‚îÇ   ‚îú‚îÄ‚îÄ query_datadog_metrics (infrastructure health)  ‚îÇ
+  ‚îÇ   ‚îî‚îÄ‚îÄ get_datadog_logs (log aggregation)             ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Redis Coordinator: ‚úÖ PHASE 1                       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Sentry Integration: ‚úÖ PHASE 2                      ‚îÇ
+  ‚îî‚îÄ‚îÄ Datadog Integration: üÜï PHASE 3                     ‚îÇ
+      ‚îú‚îÄ‚îÄ APM Tracer (distributed tracing)               ‚îÇ
+      ‚îú‚îÄ‚îÄ DogStatsD Client (metrics)                     ‚îÇ
+      ‚îú‚îÄ‚îÄ API Client (queries, dashboards)               ‚îÇ
+      ‚îî‚îÄ‚îÄ Log Correlation (trace ID injection)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Infrastructure Layer                                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Redis (localhost:6379) - Global coordination       ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Sentry (sentry.io) - Error/performance tracking    ‚îÇ
+  ‚îú‚îÄ‚îÄ Datadog (datadoghq.com) - APM, metrics, logs üÜï    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Log Files (300GB+/day) - Ripgrep/grep search       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ services.yaml (90+ services) - Service definitions ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Phase 3: Datadog Integration (NOT STARTED)

### Objective

Implement comprehensive Application Performance Monitoring (APM) and infrastructure metrics using Datadog to enable AI agents to:

1. **Diagnose Performance Issues**: Identify slow queries, bottlenecks, and latency spikes
2. **Monitor Infrastructure Health**: Track CPU, memory, disk, network metrics
3. **Trace Distributed Requests**: Follow request flow across multiple services
4. **Correlate Logs & Traces**: Link log entries to specific traces for root cause analysis
5. **Analyze System-Wide Patterns**: Detect trends, anomalies, and capacity issues

### What Datadog Provides (Beyond Sentry)

| Capability | Sentry | Datadog | Why We Need Both |
|------------|--------|---------|------------------|
| **Error Tracking** | ‚úÖ Excellent | ‚ö†Ô∏è Basic | Sentry: Best error grouping/triage |
| **Performance Monitoring** | ‚ö†Ô∏è Basic | ‚úÖ Advanced APM | Datadog: Detailed transaction tracing |
| **Infrastructure Metrics** | ‚ùå None | ‚úÖ Full Suite | CPU, memory, disk, network, custom metrics |
| **Distributed Tracing** | ‚ùå None | ‚úÖ Yes | Follow requests across microservices |
| **Log Aggregation** | ‚ùå None | ‚úÖ Yes | Centralized log search & correlation |
| **Dashboards** | ‚ö†Ô∏è Basic | ‚úÖ Advanced | Custom visualizations, alerts |
| **Alerting** | ‚úÖ Good | ‚úÖ Advanced | Datadog: More flexible alert conditions |
| **Cost** | ‚ö†Ô∏è Per-event | ‚ö†Ô∏è Per-host | Sentry: Errors, Datadog: Everything else |

**Decision**: Use Sentry for errors, Datadog for everything else (APM, metrics, logs)

---

## Implementation Tasks

### Task 3.1: Datadog SDK Integration ‚úÖ COMPLETE

**Current State**: Only config placeholders exist in `config_loader.py`:
```python
# src/config_loader.py (lines 50-52)
dd_api_key: Optional[str] = Field(default=None)
dd_app_key: Optional[str] = Field(default=None)
dd_site: str = Field(default="datadoghq.com")
```

**Step 1: Add Dependencies**

Update `pyproject.toml`:
```toml
dependencies = [
    # ... existing dependencies ...
    "ddtrace>=2.0.0",        # APM tracing
    "datadog>=0.49.0",       # API client for queries
    "datadog-api-client>=2.0.0",  # New API client
]
```

Install:
```bash
cd /home/ubuntu/elis_temp/github_projects/log-ai
~/.local/bin/uv sync
```

**Step 2: Datadog Account Setup**

1. Sign up at https://www.datadoghq.com
2. Create API Key (for metrics/APM submission)
3. Create Application Key (for querying API)
4. Get Datadog site URL (e.g., `datadoghq.com`, `datadoghq.eu`, `us5.datadoghq.com`)

**Step 3: Update Configuration**

Add to `config/.env`:
```bash
# Datadog Configuration
DATADOG_API_KEY=your_api_key_here
DATADOG_APP_KEY=your_app_key_here
DATADOG_SITE=datadoghq.com
DATADOG_SERVICE_NAME=log-ai-mcp
DATADOG_ENV=production
DATADOG_VERSION=1.0.0
```

Update `config_loader.py` (expand existing placeholders):
```python
class Settings(BaseSettings):
    # ... existing fields ...
    
    # Datadog APM Configuration
    dd_api_key: Optional[str] = Field(
        default=None,
        description="Datadog API key for metrics/APM submission"
    )
    dd_app_key: Optional[str] = Field(
        default=None,
        description="Datadog Application key for API queries"
    )
    dd_site: str = Field(
        default="datadoghq.com",
        description="Datadog site (datadoghq.com, datadoghq.eu, etc.)"
    )
    dd_service_name: str = Field(
        default="log-ai-mcp",
        description="Service name in Datadog APM"
    )
    dd_env: str = Field(
        default="production",
        description="Environment name (production, staging, development)"
    )
    dd_version: str = Field(
        default="1.0.0",
        description="Application version for release tracking"
    )
    dd_enabled: bool = Field(
        default=False,
        description="Enable Datadog integration"
    )
    
    @property
    def dd_configured(self) -> bool:
        """Check if Datadog is properly configured"""
        return bool(self.dd_api_key and self.dd_app_key and self.dd_enabled)
```

**Step 4: Create Datadog Integration Module**

Create `src/datadog_integration.py`:
```python
"""Datadog integration for APM, metrics, and log correlation"""

from ddtrace import tracer, patch_all
from datadog import initialize as dd_initialize, statsd
from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.logs_api import LogsApi
from datadog_api_client.v2.api.metrics_api import MetricsApi
from datadog_api_client.v1.api.service_level_objectives_api import ServiceLevelObjectivesApi
import logging
import sys
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta

logger = logging.getLogger("log-ai.datadog")

# Global Datadog clients
_tracer: Optional[Any] = None
_statsd_client: Optional[Any] = None
_api_client: Optional[ApiClient] = None

def init_datadog(
    api_key: str,
    app_key: str,
    site: str = "datadoghq.com",
    service_name: str = "log-ai-mcp",
    env: str = "production",
    version: str = "1.0.0"
) -> bool:
    """
    Initialize Datadog APM and metrics.
    
    Args:
        api_key: Datadog API key
        app_key: Datadog Application key
        site: Datadog site (datadoghq.com, etc.)
        service_name: Service name for APM
        env: Environment (production, staging, etc.)
        version: Application version
        
    Returns:
        True if initialization successful
    """
    global _tracer, _statsd_client, _api_client
    
    try:
        # Initialize DogStatsD for metrics
        dd_initialize(
            api_key=api_key,
            app_key=app_key,
            statsd_host='localhost',
            statsd_port=8125
        )
        _statsd_client = statsd
        
        # Configure APM tracer
        tracer.configure(
            hostname='localhost',
            port=8126,
            service=service_name,
            env=env,
            version=version,
        )
        
        # Patch async libraries for automatic tracing
        patch_all(asyncio=True, redis=True)
        _tracer = tracer
        
        # Initialize API client for queries
        configuration = Configuration()
        configuration.api_key["apiKeyAuth"] = api_key
        configuration.api_key["appKeyAuth"] = app_key
        configuration.server_variables["site"] = site
        _api_client = ApiClient(configuration)
        
        logger.info(f"[DATADOG] Initialized: service={service_name}, env={env}, site={site}")
        return True
        
    except Exception as e:
        logger.error(f"[DATADOG] Initialization failed: {e}", exc_info=True)
        return False


def trace_search_operation(
    service: str,
    pattern: str,
    time_range: Dict[str, Any]
) -> Any:
    """
    Create APM trace for log search operation.
    
    Usage:
        with trace_search_operation("hub-ca-auth", "timeout", {...}) as span:
            # Perform search
            span.set_tag("result_count", 150)
    """
    if not _tracer:
        # Return no-op context manager if not initialized
        from contextlib import nullcontext
        return nullcontext()
    
    span = _tracer.trace(
        "log_search",
        service=service,
        resource=f"search:{service}",
        span_type="custom"
    )
    span.set_tags({
        "service_name": service,
        "pattern": pattern[:100],  # Truncate long patterns
        "time_range_hours": time_range.get("hours_back", "N/A"),
    })
    return span


def record_metric(
    metric_name: str,
    value: float,
    tags: Optional[List[str]] = None,
    metric_type: str = "gauge"
) -> None:
    """
    Record custom metric to Datadog.
    
    Args:
        metric_name: Metric name (e.g., "log_ai.search.duration")
        value: Metric value
        tags: Tags for filtering (e.g., ["service:hub-ca-auth", "cached:false"])
        metric_type: gauge, count, histogram, rate
    """
    if not _statsd_client:
        return
    
    tags = tags or []
    
    try:
        if metric_type == "gauge":
            _statsd_client.gauge(metric_name, value, tags=tags)
        elif metric_type == "count":
            _statsd_client.increment(metric_name, value=int(value), tags=tags)
        elif metric_type == "histogram":
            _statsd_client.histogram(metric_name, value, tags=tags)
        elif metric_type == "rate":
            _statsd_client.rate(metric_name, value, tags=tags)
        else:
            logger.warning(f"[DATADOG] Unknown metric type: {metric_type}")
            
    except Exception as e:
        logger.error(f"[DATADOG] Failed to record metric {metric_name}: {e}")


def query_apm_traces(
    service: str,
    start_time: datetime,
    end_time: datetime,
    operation: Optional[str] = None,
    min_duration_ms: Optional[int] = None
) -> Dict[str, Any]:
    """
    Query APM traces from Datadog.
    
    Args:
        service: Service name to query
        start_time: Start time (UTC)
        end_time: End time (UTC)
        operation: Optional operation name filter
        min_duration_ms: Optional minimum duration filter
        
    Returns:
        Dict with traces and metadata
    """
    if not _api_client:
        return {"error": "Datadog not initialized"}
    
    # TODO: Implement using datadog_api_client
    # This will query the APM API for trace data
    pass


def query_metrics(
    metric_query: str,
    start_time: datetime,
    end_time: datetime
) -> Dict[str, Any]:
    """
    Query metrics from Datadog.
    
    Args:
        metric_query: Metric query string (e.g., "avg:log_ai.search.duration{service:hub-ca-auth}")
        start_time: Start time (UTC)
        end_time: End time (UTC)
        
    Returns:
        Dict with metric data
    """
    if not _api_client:
        return {"error": "Datadog not initialized"}
    
    # TODO: Implement using MetricsApi
    pass


def query_logs(
    query: str,
    start_time: datetime,
    end_time: datetime,
    limit: int = 100
) -> Dict[str, Any]:
    """
    Query logs from Datadog log aggregation.
    
    Args:
        query: Log query (Datadog query syntax)
        start_time: Start time (UTC)
        end_time: End time (UTC)
        limit: Maximum results
        
    Returns:
        Dict with log entries
    """
    if not _api_client:
        return {"error": "Datadog not initialized"}
    
    try:
        with _api_client as api_client:
            logs_api = LogsApi(api_client)
            # TODO: Implement log query
            pass
    except Exception as e:
        logger.error(f"[DATADOG] Log query failed: {e}", exc_info=True)
        return {"error": str(e)}
```

**Step 5: Test Basic Integration**

Create `tests/test_datadog_integration.py`:
```python
"""Tests for Datadog integration"""
import pytest
from src.datadog_integration import (
    init_datadog,
    trace_search_operation,
    record_metric
)
from src.config_loader import settings


@pytest.mark.skipif(not settings.dd_configured, reason="Datadog not configured")
def test_datadog_initialization():
    """Test Datadog SDK initialization"""
    result = init_datadog(
        api_key=settings.dd_api_key,
        app_key=settings.dd_app_key,
        site=settings.dd_site,
        service_name=settings.dd_service_name,
        env=settings.dd_env,
        version=settings.dd_version
    )
    assert result is True


@pytest.mark.skipif(not settings.dd_configured, reason="Datadog not configured")
def test_trace_context_manager():
    """Test APM tracing context manager"""
    with trace_search_operation("test-service", "test-pattern", {"hours_back": 1}) as span:
        assert span is not None
        span.set_tag("test", "value")


@pytest.mark.skipif(not settings.dd_configured, reason="Datadog not configured")
def test_metric_recording():
    """Test metric submission"""
    record_metric(
        "log_ai.test.metric",
        123.45,
        tags=["env:test", "service:test"],
        metric_type="gauge"
    )
    # No assertion - just ensure no exceptions
```

Run tests:
```bash
uv run pytest tests/test_datadog_integration.py -v
```

---

### Task 3.2: APM Tracing Implementation ‚úÖ COMPLETE

**Goal**: Automatically trace all log search operations with distributed context.

**Step 1: Update `server.py` to Use Tracing**

Modify `src/server.py` (in `search_logs_handler`):

```python
# Add import at top
from datadog_integration import (
    init_datadog,
    trace_search_operation,
    record_metric
)

# Initialize Datadog on startup (after Sentry/Redis initialization)
datadog_enabled = False
if settings.dd_configured:
    datadog_enabled = init_datadog(
        api_key=settings.dd_api_key,
        app_key=settings.dd_app_key,
        site=settings.dd_site,
        service_name=settings.dd_service_name,
        env=settings.dd_env,
        version=settings.dd_version
    )

# In search_logs_handler function:
async def search_logs_handler(arguments: dict) -> Sequence[types.TextContent]:
    service = arguments.get("service_name")
    pattern = arguments.get("query")
    time_range = {...}  # Extract time range
    
    # Start APM trace
    with trace_search_operation(service, pattern, time_range) as span:
        try:
            # ... existing search logic ...
            
            # Add trace tags
            if span:
                span.set_tags({
                    "result_count": len(matches),
                    "cached": cached,
                    "duration_ms": duration_ms,
                    "overflow": overflow,
                })
            
            # Record metrics
            record_metric(
                "log_ai.search.duration",
                duration_ms,
                tags=[
                    f"service:{service}",
                    f"cached:{cached}",
                    f"overflow:{overflow}"
                ],
                metric_type="histogram"
            )
            
            record_metric(
                "log_ai.search.result_count",
                len(matches),
                tags=[f"service:{service}"],
                metric_type="histogram"
            )
            
            return results
            
        except Exception as e:
            if span:
                span.set_tag("error", True)
                span.set_tag("error.message", str(e))
            raise
```

**Step 2: Add Tracing to Redis Operations**

Update `src/redis_coordinator.py`:
```python
from ddtrace import tracer

class RedisCache:
    async def get(self, key: str) -> Optional[str]:
        with tracer.trace("redis.cache.get", service="log-ai-redis") as span:
            span.set_tag("cache_key", key[:50])  # Truncate
            result = await self.redis.get(key)
            span.set_tag("hit", result is not None)
            return result
    
    async def set(self, key: str, value: str, ttl: int):
        with tracer.trace("redis.cache.set", service="log-ai-redis") as span:
            span.set_tag("cache_key", key[:50])
            span.set_tag("ttl_seconds", ttl)
            await self.redis.setex(key, ttl, value)
```

---

### Task 3.3: Metrics Collection ‚ùå NOT STARTED

**Goal**: Track infrastructure and application metrics.

**Metrics to Collect**:

1. **Search Metrics**:
   - `log_ai.search.duration` (histogram) - Search execution time
   - `log_ai.search.result_count` (histogram) - Number of matches
   - `log_ai.search.cache_hit_rate` (gauge) - Cache effectiveness
   - `log_ai.search.overflow_rate` (gauge) - % of searches with overflow

2. **System Metrics**:
   - `log_ai.redis.connection_pool` (gauge) - Active Redis connections
   - `log_ai.semaphore.available` (gauge) - Available search slots
   - `log_ai.file_output.size_bytes` (histogram) - Overflow file sizes

3. **Error Metrics**:
   - `log_ai.errors.total` (count) - Total errors by type
   - `log_ai.timeouts.total` (count) - Search timeouts

**Implementation**:

Add metrics collection throughout `server.py`:
```python
# After search completes
record_metric(
    "log_ai.cache.hit_rate",
    cache_hit_rate,
    tags=["service:log-ai"],
    metric_type="gauge"
)

# Track semaphore usage
record_metric(
    "log_ai.semaphore.available",
    global_search_semaphore._value,  # Available slots
    tags=["limit:20"],
    metric_type="gauge"
)

# Track overflow file size
if overflow:
    file_size = os.path.getsize(file_path)
    record_metric(
        "log_ai.overflow.file_size_bytes",
        file_size,
        tags=[f"service:{service}"],
        metric_type="histogram"
    )
```

---

### Task 3.4: Infrastructure Monitoring ‚ùå NOT STARTED

**Goal**: Monitor syslog server infrastructure health.

**Step 1: Install Datadog Agent on Syslog Server**

```bash
# On syslog.example.com
DD_API_KEY=your_api_key_here \
DD_SITE="datadoghq.com" \
bash -c "$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script_agent7.sh)"

# Start agent
sudo systemctl start datadog-agent
sudo systemctl enable datadog-agent
```

**Step 2: Configure Agent for Log-AI**

Create `/etc/datadog-agent/conf.d/log_ai.yaml`:
```yaml
init_config:

instances:
  - host: localhost
    port: 8125  # DogStatsD port
    
logs:
  - type: file
    path: /tmp/log-ai/*.log
    service: log-ai-mcp
    source: python
    tags:
      - env:production
```

**Step 3: Configure Log Collection**

Update `/etc/datadog-agent/datadog.yaml`:
```yaml
logs_enabled: true
process_config:
  enabled: "true"
apm_config:
  enabled: true
  apm_non_local_traffic: false
```

Restart agent:
```bash
sudo systemctl restart datadog-agent
```

**Step 4: Verify Integration**

```bash
# Check agent status
sudo datadog-agent status

# Test metrics submission
echo "custom_metric:60|g|#shell" | nc -4u -w0 localhost 8125

# View live metrics in Datadog UI
# Navigate to: Metrics ‚Üí Explorer ‚Üí Search "log_ai.*"
```

---

### Task 3.5: Log Aggregation ‚ùå NOT STARTED

**Goal**: Send MCP server logs to Datadog for centralized search and correlation with traces.

**Step 1: Configure Python Logging to Send to Datadog**

Update `src/server.py` logging setup:
```python
import logging
from logging.handlers import SocketHandler
import json

class DatadogLogHandler(logging.Handler):
    """Custom handler to send logs to Datadog"""
    
    def __init__(self, api_key: str, service: str):
        super().__init__()
        self.api_key = api_key
        self.service = service
    
    def emit(self, record: logging.LogRecord):
        """Send log record to Datadog"""
        try:
            log_entry = {
                "message": self.format(record),
                "level": record.levelname,
                "timestamp": int(record.created * 1000),  # milliseconds
                "service": self.service,
                "ddsource": "python",
                "ddtags": f"env:production,service:{self.service}",
            }
            
            # Add trace context if available
            if hasattr(record, 'dd.trace_id'):
                log_entry["dd.trace_id"] = getattr(record, 'dd.trace_id')
                log_entry["dd.span_id"] = getattr(record, 'dd.span_id')
            
            # Send to Datadog HTTP API
            # TODO: Implement async HTTP POST to Datadog logs API
            
        except Exception:
            self.handleError(record)

# Add handler if Datadog enabled
if datadog_enabled:
    dd_handler = DatadogLogHandler(
        api_key=settings.dd_api_key,
        service=settings.dd_service_name
    )
    logger.addHandler(dd_handler)
```

**Step 2: Inject Trace Context into Logs**

Use `ddtrace` to automatically add trace IDs:
```python
from ddtrace import tracer
import logging

# Logs will automatically include trace_id and span_id when tracing is active
logger = logging.getLogger("log-ai")
logger.info("Search completed")  # Automatically includes trace context
```

---

### Task 3.6: MCP Tools for Datadog Queries ‚ùå NOT STARTED

**Goal**: Expose Datadog data to AI agents via MCP tools.

**New MCP Tools**:

1. **query_datadog_apm** - Query APM traces
2. **get_datadog_traces** - Get trace details with spans
3. **query_datadog_metrics** - Query infrastructure metrics
4. **get_datadog_logs** - Search Datadog logs

**Implementation in `server.py`**:

```python
@mcp.tool()
async def query_datadog_apm(
    service: str,
    hours_back: int = 1,
    operation: Optional[str] = None,
    min_duration_ms: Optional[int] = None,
    format: str = "text"
) -> str:
    """
    Query Datadog APM traces for performance analysis.
    
    Args:
        service: Service name (e.g., "hub-ca-auth")
        hours_back: How many hours to look back (default: 1)
        operation: Filter by operation name (e.g., "log_search")
        min_duration_ms: Minimum duration filter for slow traces
        format: Response format ("text" or "json")
    
    Returns:
        Trace data with performance metrics
    """
    from datadog_integration import query_apm_traces
    from datetime import datetime, timedelta
    
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=hours_back)
    
    result = query_apm_traces(
        service=service,
        start_time=start_time,
        end_time=end_time,
        operation=operation,
        min_duration_ms=min_duration_ms
    )
    
    if format == "json":
        return json.dumps(result, indent=2)
    
    # Format as human-readable text
    if "error" in result:
        return f"Error: {result['error']}"
    
    traces = result.get("traces", [])
    output = [
        f"=== APM Traces: {service} ===",
        f"Time Range: {start_time} - {end_time} UTC",
        f"Total Traces: {len(traces)}",
        ""
    ]
    
    for trace in traces[:20]:  # Show top 20
        output.append(
            f"[{trace['timestamp']}] {trace['operation']} "
            f"({trace['duration_ms']}ms) - {trace['resource']}"
        )
    
    return "\n".join(output)


@mcp.tool()
async def query_datadog_metrics(
    metric_query: str,
    hours_back: int = 1,
    format: str = "text"
) -> str:
    """
    Query Datadog metrics for infrastructure monitoring.
    
    Args:
        metric_query: Datadog metric query (e.g., "avg:system.cpu.user{host:syslog}")
        hours_back: How many hours to look back
        format: Response format ("text" or "json")
    
    Returns:
        Metric data with timestamps
    """
    from datadog_integration import query_metrics
    from datetime import datetime, timedelta
    
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=hours_back)
    
    result = query_metrics(
        metric_query=metric_query,
        start_time=start_time,
        end_time=end_time
    )
    
    if format == "json":
        return json.dumps(result, indent=2)
    
    # Format as human-readable text
    return format_metric_response(result)


@mcp.tool()
async def get_datadog_logs(
    query: str,
    hours_back: int = 1,
    limit: int = 100,
    format: str = "text"
) -> str:
    """
    Search Datadog aggregated logs.
    
    Args:
        query: Datadog log query (e.g., "service:hub-ca-auth status:error")
        hours_back: How many hours to look back
        limit: Maximum results (default: 100)
        format: Response format ("text" or "json")
    
    Returns:
        Log entries matching query
    """
    from datadog_integration import query_logs
    from datetime import datetime, timedelta
    
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=hours_back)
    
    result = query_logs(
        query=query,
        start_time=start_time,
        end_time=end_time,
        limit=limit
    )
    
    if format == "json":
        return json.dumps(result, indent=2)
    
    # Format as human-readable text
    return format_log_response(result)
```

---

### Task 3.7: Dashboard Setup ‚ùå NOT STARTED

**Goal**: Create Datadog dashboards for AI agents to reference.

**Dashboard 1: LogAI MCP Server Overview**

Widgets:
1. **Search Performance**
   - Metric: `avg:log_ai.search.duration` by service
   - Visualization: Timeseries
   
2. **Cache Hit Rate**
   - Metric: `avg:log_ai.cache.hit_rate`
   - Visualization: Query value (gauge)
   
3. **Active Searches**
   - Metric: `20 - avg:log_ai.semaphore.available`
   - Visualization: Timeseries
   
4. **Error Rate**
   - Metric: `sum:log_ai.errors.total`
   - Visualization: Timeseries with alert threshold

5. **Top Slow Searches**
   - APM Service map
   - Filter: `operation:log_search` with `duration >5s`

**Dashboard 2: Syslog Server Infrastructure**

Widgets:
1. **CPU Usage**
   - Metric: `avg:system.cpu.user{host:syslog}`
   
2. **Memory Usage**
   - Metric: `avg:system.mem.used{host:syslog}`
   
3. **Disk I/O**
   - Metric: `avg:system.io.read_bytes{host:syslog}`
   
4. **Network Traffic**
   - Metric: `avg:system.net.bytes_rcvd{host:syslog}`

**Export as Terraform** for version control:
```bash
# Export dashboard JSON
curl -X GET "https://api.datadoghq.com/api/v1/dashboard/{dashboard_id}" \
     -H "DD-API-KEY: ${DD_API_KEY}" \
     -H "DD-APPLICATION-KEY: ${DD_APP_KEY}" > dashboard.json
```

---

### Task 3.8: Alert Configuration ‚ùå NOT STARTED

**Goal**: Configure alerts for critical issues.

**Alert 1: High Search Duration**
```yaml
name: "LogAI - Slow Search Performance"
query: "avg(last_5m):avg:log_ai.search.duration{*} > 10000"
message: |
  Search duration exceeded 10 seconds (p95).
  
  Service: {{service.name}}
  Duration: {{value}}ms
  
  @slack-logai-alerts
type: metric alert
threshold: 10000  # 10 seconds
```

**Alert 2: Low Cache Hit Rate**
```yaml
name: "LogAI - Cache Hit Rate Low"
query: "avg(last_15m):avg:log_ai.cache.hit_rate{*} < 50"
message: |
  Cache hit rate dropped below 50%.
  
  Current rate: {{value}}%
  Expected: >70%
  
  Check Redis connection and cache TTL settings.
type: metric alert
threshold: 50
```

**Alert 3: High Error Rate**
```yaml
name: "LogAI - High Error Rate"
query: "sum(last_5m):sum:log_ai.errors.total{*}.as_rate() > 10"
message: |
  Error rate exceeded 10 errors/min.
  
  Error count: {{value}}
  
  Check Sentry for error details: https://sentry.io/...
type: metric alert
threshold: 10
```

**Alert 4: Semaphore Exhaustion**
```yaml
name: "LogAI - Search Queue Full"
query: "avg(last_5m):avg:log_ai.semaphore.available{*} < 2"
message: |
  Search semaphore nearly exhausted (<2 slots available).
  
  Available slots: {{value}}
  Total capacity: 20
  
  Check for stuck searches or increase capacity.
type: metric alert
threshold: 2
```

---

### Task 3.9: Integration Testing ‚ùå NOT STARTED

**Goal**: End-to-end testing of Datadog integration.

Create `tests/test_datadog_e2e.py`:
```python
"""End-to-end tests for Datadog integration"""
import pytest
import asyncio
from datetime import datetime, timedelta
from src.server import search_logs_handler
from src.datadog_integration import query_apm_traces, query_metrics
from src.config_loader import settings


@pytest.mark.asyncio
@pytest.mark.skipif(not settings.dd_configured, reason="Datadog not configured")
async def test_search_creates_trace():
    """Test that log search creates APM trace"""
    
    # Perform search
    result = await search_logs_handler({
        "service_name": "hub-ca-auth",
        "query": "test",
        "hours_back": 1
    })
    
    # Wait for trace to be processed
    await asyncio.sleep(5)
    
    # Query for trace
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(minutes=5)
    
    traces = query_apm_traces(
        service="hub-ca-auth",
        start_time=start_time,
        end_time=end_time,
        operation="log_search"
    )
    
    assert len(traces["traces"]) > 0
    assert traces["traces"][0]["operation"] == "log_search"


@pytest.mark.asyncio
@pytest.mark.skipif(not settings.dd_configured, reason="Datadog not configured")
async def test_metrics_recorded():
    """Test that search metrics are recorded"""
    
    # Perform search
    await search_logs_handler({
        "service_name": "hub-ca-auth",
        "query": "test",
        "hours_back": 1
    })
    
    # Wait for metric submission
    await asyncio.sleep(5)
    
    # Query metric
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(minutes=5)
    
    metrics = query_metrics(
        metric_query="avg:log_ai.search.duration{service:hub-ca-auth}",
        start_time=start_time,
        end_time=end_time
    )
    
    assert "series" in metrics
    assert len(metrics["series"]) > 0


@pytest.mark.asyncio
@pytest.mark.skipif(not settings.dd_configured, reason="Datadog not configured")
async def test_mcp_datadog_tools():
    """Test MCP tools for querying Datadog"""
    from src.server import query_datadog_apm, query_datadog_metrics
    
    # Test APM query
    apm_result = await query_datadog_apm(
        service="hub-ca-auth",
        hours_back=1,
        format="json"
    )
    assert "traces" in apm_result or "error" in apm_result
    
    # Test metrics query
    metrics_result = await query_datadog_metrics(
        metric_query="avg:log_ai.search.duration{*}",
        hours_back=1,
        format="json"
    )
    assert "series" in metrics_result or "error" in metrics_result
```

Run integration tests:
```bash
# Set Datadog credentials in test environment
export DATADOG_API_KEY=your_key
export DATADOG_APP_KEY=your_app_key
export DATADOG_ENABLED=true

uv run pytest tests/test_datadog_e2e.py -v --log-cli-level=INFO
```

---

### Task 3.10: Documentation Update ‚ùå NOT STARTED

**Goal**: Document Datadog integration for users and agents.

**Update Files**:

1. **README.md** - Add Datadog section
2. **docs/IMPLEMENTATION.md** - Add Phase 3 completion details
3. **docs/QUICK_REFERENCE.md** - Add Datadog tool examples
4. **docs/INDEX.md** - Move plan to implemented/

**New Documentation**: Create `docs/DATADOG_SETUP.md`:
```markdown
# Datadog Setup Guide

## Prerequisites
- Datadog account (free trial available)
- API Key and Application Key
- Datadog Agent installed on syslog server

## Configuration

1. **Get Datadog Keys**:
   ```bash
   # Login to https://app.datadoghq.com
   # Navigate to: Organization Settings ‚Üí API Keys
   # Copy API Key
   
   # Navigate to: Organization Settings ‚Üí Application Keys
   # Create new Application Key
   # Copy Application Key
   ```

2. **Update config/.env**:
   ```bash
   DATADOG_API_KEY=your_api_key_here
   DATADOG_APP_KEY=your_app_key_here
   DATADOG_SITE=datadoghq.com
   DATADOG_SERVICE_NAME=log-ai-mcp
   DATADOG_ENV=production
   DATADOG_ENABLED=true
   ```

3. **Install Datadog Agent** (on syslog server):
   ```bash
   DD_API_KEY=your_api_key \
   DD_SITE="datadoghq.com" \
   bash -c "$(curl -L https://s3.amazonaws.com/dd-agent/scripts/install_script_agent7.sh)"
   ```

4. **Verify Installation**:
   ```bash
   sudo datadog-agent status
   ```

## Using Datadog Tools in VS Code/Cursor

### Query APM Traces
```
Can you check Datadog APM for slow searches in hub-ca-auth over the last hour?
```

Agent will call:
```json
{
  "tool": "query_datadog_apm",
  "service": "hub-ca-auth",
  "hours_back": 1,
  "min_duration_ms": 5000
}
```

### Query Infrastructure Metrics
```
What's the CPU usage on the syslog server?
```

Agent will call:
```json
{
  "tool": "query_datadog_metrics",
  "metric_query": "avg:system.cpu.user{host:syslog}",
  "hours_back": 1
}
```

### Search Datadog Logs
```
Find error logs from hub-ca-auth in Datadog
```

Agent will call:
```json
{
  "tool": "get_datadog_logs",
  "query": "service:hub-ca-auth status:error",
  "hours_back": 1,
  "limit": 100
}
```

## Dashboards

Access dashboards at:
- LogAI MCP Overview: https://app.datadoghq.com/dashboard/...
- Syslog Infrastructure: https://app.datadoghq.com/dashboard/...

## Troubleshooting

### Agent Not Running
```bash
sudo systemctl restart datadog-agent
sudo datadog-agent status
```

### No Traces Appearing
- Check APM is enabled in datadog.yaml
- Verify ddtrace is installed: `uv run python -c "import ddtrace; print(ddtrace.__version__)"`
- Check agent logs: `sudo tail -f /var/log/datadog/agent.log`

### Metrics Not Submitting
- Test DogStatsD: `echo "custom_metric:60|g" | nc -4u -w0 localhost 8125`
- Check firewall: `sudo ufw status`
- Verify API key is correct
```

---

## Success Criteria

Phase 3 will be considered **COMPLETE** when:

- [ ] ‚úÖ All dependencies installed (`ddtrace`, `datadog`, `datadog-api-client`)
- [ ] ‚úÖ Datadog SDK initialized in `server.py`
- [ ] ‚úÖ APM tracing active for all search operations
- [ ] ‚úÖ Metrics recorded for searches, cache, semaphore, errors
- [ ] ‚úÖ Datadog Agent installed and running on syslog server
- [ ] ‚úÖ Logs forwarded to Datadog with trace correlation
- [ ] ‚úÖ 4 new MCP tools implemented:
  - `query_datadog_apm`
  - `get_datadog_traces`
  - `query_datadog_metrics`
  - `get_datadog_logs`
- [ ] ‚úÖ 2 dashboards created in Datadog UI
- [ ] ‚úÖ 4 alert rules configured and tested
- [ ] ‚úÖ Integration tests passing (3/3)
- [ ] ‚úÖ Documentation updated (README, IMPLEMENTATION, new DATADOG_SETUP.md)
- [ ] ‚úÖ Plan moved to `docs/plans/implemented/`

---

## Dependencies & Prerequisites

### Software Requirements
- Python 3.12+
- Redis (already installed from Phase 1) ‚úÖ
- Datadog account (free trial available) ‚ùå NOT SETUP
- Datadog Agent 7.x ‚ùå NOT INSTALLED

### Configuration Requirements
- Datadog API Key ‚ùå NOT CONFIGURED
- Datadog Application Key ‚ùå NOT CONFIGURED
- Syslog server SSH access ‚úÖ AVAILABLE

### Estimated Effort
- Development: 3-4 days
- Testing: 1-2 days
- Documentation: 1 day
- **Total**: ~5-7 days

---

## Rollout Strategy

### Phase 3.1: Development Environment (Local Testing)
1. Setup Datadog trial account
2. Install dependencies locally
3. Implement basic APM tracing
4. Test with local MCP server

### Phase 3.2: Staging Environment
1. Install Datadog Agent on staging syslog server
2. Deploy Phase 3 code to staging
3. Run integration tests
4. Validate dashboards and alerts

### Phase 3.3: Production Rollout
1. Create production Datadog API keys
2. Install Datadog Agent on production syslog server
3. Deploy to production with feature flag (`DATADOG_ENABLED=false`)
4. Enable gradually (monitor for issues)
5. Full rollout after 48 hours of stability

---

## Monitoring Plan

### During Implementation
- **Daily**: Check test coverage, ensure no regressions
- **Weekly**: Review progress against task checklist

### Post-Deployment
- **First 24 hours**: Watch error rates, agent health, metric submission
- **First week**: Validate alert triggers, dashboard accuracy
- **First month**: Analyze APM data for optimization opportunities

---

## Rollback Plan

If Phase 3 causes issues:

1. **Immediate**: Set `DATADOG_ENABLED=false` in `config/.env`
2. **Graceful degradation**: System continues working without Datadog
3. **Redis/Sentry**: Unaffected (Phase 1 & 2 remain functional)
4. **Code revert**: Git revert to pre-Phase-3 commit if needed

**Critical**: Phase 3 is **additive only** - no breaking changes to existing features.

---

## Related Documentation

- [AGENTS.md](../../AGENTS.md) - AI Agent instructions
- [IMPLEMENTATION.md](../IMPLEMENTATION.md) - Architecture details
- [INDEX.md](../INDEX.md) - Documentation hub
- [Implemented Plans](../plans/implemented/) - Previous phases

---

## Notes for AI Agents

**When working on this plan**:
1. Check off tasks in frontmatter `completion` list as you complete them
2. Update `date_updated` in frontmatter
3. Update [docs/INDEX.md](../INDEX.md) with progress percentage
4. Test thoroughly - all existing tests must continue passing
5. When 100% complete, move this file to `docs/plans/implemented/`

**Testing Philosophy**:
- Unit tests for each function
- Integration tests for API interactions
- E2E tests for MCP tool functionality
- Manual verification in Datadog UI

**Code Quality Standards**:
- Type hints on all functions
- Docstrings (Google style)
- Error handling with graceful degradation
- Async-first patterns
- Configuration via `settings` object (no hardcoded values)

---

**Last Updated**: 2026-01-24  
**Next Review**: After Task 3.1 completion
